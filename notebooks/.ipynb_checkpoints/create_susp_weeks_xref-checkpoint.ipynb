{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d323ab",
   "metadata": {},
   "source": [
    "### Author: Rodolfo Elenes\n",
    "\n",
    "Date Created: 8/14/2025\n",
    "\n",
    "Change log:\n",
    "8/14/2025 - Initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aeb6f",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ead45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import duckdb\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fade071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f972d8",
   "metadata": {},
   "source": [
    "##### Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2623bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_wiki_tbl(con_memory):\n",
    "#   Function name: construct_wiki_tbl\n",
    "#   Description: Create the base of the Wikipedia table\n",
    "#   Parameters: con_memory\n",
    "#        con_memory(ducbdb object): used to carry duckdb queries\n",
    "#   Return values: wiki\n",
    "#        wiki(pandas dataframe): The dataframe with all suspension data from Wikipedia\n",
    "\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_suspensions_in_the_NFL\"\n",
    "    print(\"Attempting to construct Wikipedia table from:\", url)\n",
    "    # Scrape data from Wikipedia\n",
    "    columns = [\"Date suspended\", \"Suspension length\", \"Name\", \"Position\", \"Team at the time of suspension\"]\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "    for i in [0, 1]: # Gather Player suspensions + Suspensions for violating the substance policies\n",
    "        time.sleep(6) # Respect website scraping policies\n",
    "        df_temp = scrape_web_src(url, i)\n",
    "        df_temp = df_temp[columns]\n",
    "        df = pd.concat([df, df_temp])\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date suspended'], format=\"%B %d, %Y\", errors='coerce')\n",
    "    df['Date'] = np.where(df.Date.isnull(), pd.to_datetime(df['Date suspended'], format=\"%B %Y\", errors='coerce'), df.Date)\n",
    "\n",
    "    # Column renaming\n",
    "    edit_df_cols = df.columns.tolist()\n",
    "    final_columns = ['Date', 'Susp_len', 'Player', 'Position', 'Team', 'Drop']\n",
    "    for i in range(df.shape[1]):\n",
    "        edit_df_cols[i] = final_columns[i]\n",
    "    df.columns = edit_df_cols\n",
    "\n",
    "    # Dataframe transformations\n",
    "    df['Date'] = df['Drop']\n",
    "    df = df.drop('Drop', axis=1)\n",
    "    df['Position'] = np.where(df['Position'].isin(['FB', 'HB']), 'RB', df.Position) # THINK ABOUT: u want to convert positions to RB\n",
    "    df = df[(df['Date'] >= '1990') & (df['Position'] == 'RB') & (~df.Susp_len.str.contains('overturned', case=False))]\n",
    "\n",
    "    # Hard code fixes\n",
    "    df['Susp_len'] = np.where(df.Susp_len == 'Indefinite (reinstated in Feb. 2015)[116]', 'Entire 2014 season', df.Susp_len)\n",
    "    df['Susp_len'] = np.where(df.Susp_len == 'Indefinite[d] (reinstated four days later)[133]', '1 games', df.Susp_len)\n",
    "    df['Susp_len'] = np.where(df.Susp_len == 'Indefinite (reinstated in Aug. 2012)[311]', 'Indefinite', df.Susp_len)\n",
    "    df['Susp_len'] = np.where(df.Susp_len == 'Indefinite (reinstated in Dec. 2016)[532]', 'Entire 2016 season', df.Susp_len)\n",
    "    df['Susp_len'] = np.where(df.Susp_len == 'Indefinite', \"Entire \" + df['Date'].dt.year.astype(str) + \" season\", df.Susp_len)\n",
    "    df['Susp_len'] = np.where(df.Susp_len == '3 games (later reduced to 2 games)[476]', '2 games', df.Susp_len)\n",
    "\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Get rows with \"Entire\" in Susp_len\n",
    "    df2 = df.query(\"Susp_len.str.contains('entire', case=False)\")\n",
    "    susp_list1 = df2.Susp_len.unique().tolist()\n",
    "\n",
    "    # Get rows with only number of game suspensions defined\n",
    "    df3 = df[(~df['Susp_len'].isin(susp_list1))]\n",
    "    susp_list2 = df3.Susp_len.tolist()\n",
    "    fltrd_susp_list2 = []\n",
    "    for i in susp_list2: # Remove inconsistent games/games[d] phrasing\n",
    "        fltrd_susp_list2.append(i.split(\" \")[0])    \n",
    "    df3['Susp_len'] = fltrd_susp_list2\n",
    "    df3['Susp_len'] = df3.Susp_len + \" games\" # Added back games for consistency\n",
    "\n",
    "    wiki = pd.concat([df2, df3])\n",
    "    \n",
    "    # Attach Season and Week information to the suspended players\n",
    "    df_dates = pd.read_csv(\"../tables/nfl_dates_xref.csv\")\n",
    "    wiki = con_memory.execute(\"SELECT * FROM wiki JOIN df_dates ON df_dates.Date = wiki.Date\").fetchdf()\n",
    "    wiki['Date'] = pd.to_datetime(wiki.Date, format='%Y-%m-%d')\n",
    "    print(\"Wikipedia table constructed.\")\n",
    "    \n",
    "    return wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pst_tbl(con_memory):\n",
    "#   Function name: construct_pst_tbl\n",
    "#   Description: Create the base of the PST table\n",
    "#   Parameters: con_memory\n",
    "#        con_memory(ducbdb object): used to carry duckdb queries\n",
    "#   Return values: df\n",
    "#        df(pandas dataframe): The dataframe with all suspension data from prosportstransactions.com\n",
    "\n",
    "    src_tbl = \"../src/nfl_suspensions_by_pst.csv\"\n",
    "    print(\"Attempting to construct Prosportstransactions.com table from:\", src_tbl)\n",
    "    df_roster = construct_df_roster(con_memory)\n",
    "    df_roster = df_roster[['Season', 'Player', 'ABV', 'Position']]\n",
    "    df_teams = construct_df_teams()\n",
    "\n",
    "    # gather all pfr_abv\n",
    "    pfr_abvs = df_teams.PFR_ABV.unique().tolist()\n",
    "\n",
    "    # Take tmleg and make a list of seasons\n",
    "    df_teams_exp = pd.DataFrame()\n",
    "    for franchise in pfr_abvs:\n",
    "        df_temp = df_teams[(df_teams.PFR_ABV == franchise)]\n",
    "        df_temp['TmLegacy'] = np.where(df_temp.TmLegacy == '', '-2024', df_temp.TmLegacy)\n",
    "        df_temp['strt_yr'] = df_temp.TmLegacy.str.split(\"-\").str[0]\n",
    "        df_temp['end_yr'] = df_temp.TmLegacy.str.split(\"-\").str[1]\n",
    "        df_temp = df_temp.sort_values('end_yr').reset_index(drop=True)\n",
    "\n",
    "        yrs = sorted(df_temp.strt_yr.unique().tolist() + df_temp.end_yr.unique().tolist() + ['1990'])\n",
    "        yrs.remove('')\n",
    "        if yrs == ['1990', '2024']:\n",
    "            df_temp['strt_yr'] = '1990'\n",
    "\n",
    "        # Take the last row, then take the row before that and get the end_yr value\n",
    "        if df_temp.shape[0] > 1:\n",
    "            last_row = df_temp.iloc[-1]\n",
    "            prev_end_yr = df_temp.iloc[last_row.name - 1]['end_yr']\n",
    "            last_row['strt_yr'] = int(prev_end_yr) + 1\n",
    "\n",
    "        # Explode out the rows for each season per team\n",
    "        df_temp['strt_yr'] = pd.to_numeric(df_temp.strt_yr).astype(int)\n",
    "        df_temp['end_yr'] = pd.to_numeric(df_temp.end_yr).astype(int)\n",
    "        df_temp = (df_temp.assign(Season=df_temp.apply(lambda r: list(range(r[\"strt_yr\"], r[\"end_yr\"] + 1)), axis=1))\n",
    "          .explode(\"Season\", ignore_index=True)[[\"Team\", \"ABV\", \"PFR_ABV\", \"short_name\", \"Season\"]])\n",
    "\n",
    "        df_teams_exp = pd.concat([df_teams_exp, df_temp]).reset_index(drop=True) # df teams expanded contains seasons each \n",
    "                                                                                  # franchise was active in row by row format\n",
    "\n",
    "    # Join df_teams_exp with df_roster to have a records of all players and the correct team identifiers\n",
    "    df = con_memory.execute(\"\"\"SELECT df_teams_exp.Season, Team, short_name, df_teams_exp.ABV, PFR_ABV, Player, Position FROM df_teams_exp \n",
    "                               JOIN df_roster ON df_teams_exp.ABV = df_roster.ABV \n",
    "                               AND df_teams_exp.Season = df_roster.Season\"\"\").fetchdf()\n",
    "    df = df.drop('Season', axis=1).drop_duplicates()\n",
    "    con_memory.register('roster', df)\n",
    "    \n",
    "    # Construct suspensions data from Prosportstransactions.com and join with rosters table to bring identifiers to suspended players\n",
    "    df = pd.read_csv(src_tbl)\n",
    "    df = df[['Date', 'Team', 'Acquired', 'Relinquished', 'Notes']]\n",
    "    df['Acquired'] = np.where(df.Acquired == '#VALUE!', np.nan, df.Acquired)\n",
    "    df['Relinquished'] = np.where(df.Relinquished == '#VALUE!', np.nan, df.Relinquished)\n",
    "    df['Team'] = np.where(df.Team.isnull(), 'Free Agent', df.Team)\n",
    "    df['Date'] = pd.to_datetime(df.Date)\n",
    "    df['name'] = np.where(df.Acquired.isnull() == False, df.Acquired, np.nan)\n",
    "    df['name'] = np.where(df.Relinquished.isnull() == False, df.Relinquished, np.nan)\n",
    "    df = con_memory.execute(\"SELECT * FROM roster JOIN df ON roster.Player = df.name AND roster.short_name = df.Team\").fetchdf()\n",
    "    con_memory.register('pst', df)\n",
    "\n",
    "    # Attach Season and Week information to the suspended players and filter out fines entries\n",
    "    df_dates = pd.read_csv(\"../tables/nfl_dates_xref.csv\")\n",
    "    df = con_memory.execute(\"\"\"SELECT Season, Week, Date, Team, Player, Notes AS Susp_len FROM \n",
    "                               (SELECT * FROM df_dates JOIN pst ON df_dates.Date = pst.Date) \n",
    "                               WHERE Notes NOT ILIKE '%fine%' \n",
    "                               ORDER BY Date\"\"\").fetchdf()\n",
    "\n",
    "    # Extract suspension lengths\n",
    "    filter_list = df.Susp_len.unique()\n",
    "    pattern = r\"\\b\\d+\\s+games?\\b\"\n",
    "    matches = []\n",
    "    for text in filter_list:\n",
    "        found = re.findall(pattern, text.lower())\n",
    "        if found:\n",
    "            matches.extend(found)\n",
    "            df['Susp_len'] = np.where(df.Susp_len == text, found[0], df.Susp_len)\n",
    "    df['Date'] = pd.to_datetime(df.Date, format='%Y-%m-%d')\n",
    "    df['Susp_len'] = np.where(df['Susp_len'].str.contains('indef'), \"Entire \" + df['Date'].dt.year.astype(str) + \" season\", df.Susp_len)\n",
    "    df = df[(df['Susp_len'] != 'suspended from practice squad')] # Not including practice squad suspensions\n",
    "    print(\"PST table constructed.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f410f45",
   "metadata": {},
   "source": [
    "##### Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5bf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_web_src(url, tbl=0):\n",
    "#   Function name: scrape_web_src\n",
    "#   Description: This function is used to provide a stronger GET request to scrape web data\n",
    "#   Parameters: url, tbl\n",
    "#        url(str): The target URL\n",
    "#        tbl(int): The target table from the scraped web source\n",
    "#   Return values: df\n",
    "#        tables(pandas dataframe): The target table from the URL\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        tables = pd.read_html(response.text)\n",
    "        tables = tables[tbl]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_tbl_corrections(df):\n",
    "#   Function name: final_tbl_corrections\n",
    "#   Description: Make manual edits to the final table to ensure a smoother process for later scripts\n",
    "#   Parameters: df\n",
    "#        df(pandas dataframe): Final table to be edited\n",
    "#   Return values: df\n",
    "#        df(pandas dataframe): Final table with edits\n",
    "    \n",
    "    # Ezekiel Elliot 2017 suspension, refined to one entry for the final table:\n",
    "    drop_indx = df[(df.Player == 'Ezekiel Elliott') & (df.Date == '2017-08-11')].index[0]\n",
    "    df = df.drop(drop_indx)\n",
    "    print(\"\\nManually removed Ezekiel Elliott[2017-08-11] 6 game suspension entry \\nas the suspension was reinstated later on for weeks 10-15 of 2017 season.\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922c96f",
   "metadata": {},
   "source": [
    "##### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#   Function name: main\n",
    "#   Description: The entry function of the notebook\n",
    "\n",
    "    con_memory = duckdb.connect(database=':memory:')\n",
    "    wiki = construct_wiki_tbl(con_memory)\n",
    "    suspensions_pst = construct_pst_tbl(con_memory)\n",
    "    \n",
    "    # Union the Wikipedia and PST data to get a unique table with all suspension entries publicly available\n",
    "    df = con_memory.execute(\"\"\"SELECT * FROM \n",
    "                               (SELECT * FROM suspensions_pst \n",
    "                               UNION \n",
    "                               SELECT Season, Week, Date, Team, Player, Susp_len FROM wiki)\n",
    "                               ORDER BY Date, Player\"\"\").fetchdf()\n",
    "    df = final_tbl_corrections(df)\n",
    "    save_df(df, '../tables', 'susp_weeks_xref.csv')\n",
    "    display(df)\n",
    "    con_memory.close()\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFL",
   "language": "python",
   "name": "nfl_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
